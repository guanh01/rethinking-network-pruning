{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hguan2/anaconda2/envs/distiller/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f87d807a130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import distiller \n",
    "import argparse\n",
    "import numpy as np\n",
    "import os, collections\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import time \n",
    "\n",
    "import models\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Slimming CIFAR training')\n",
    "parser.add_argument('--dataset', type=str, default='cifar100',\n",
    "                    help='training dataset (default: cifar100)')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=256, metavar='N',\n",
    "                    help='input batch size for testing (default: 256)')\n",
    "parser.add_argument('--epochs', type=int, default=160, metavar='N',\n",
    "                    help='number of epochs to train (default: 160)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate (default: 0.1)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save', default='./logs', type=str, metavar='PATH',\n",
    "                    help='path to save prune model (default: current directory)')\n",
    "parser.add_argument('--arch', default='vgg', type=str, \n",
    "                    help='architecture to use')\n",
    "parser.add_argument('--depth', default=16, type=int,\n",
    "                    help='depth of the neural network')\n",
    "print('using GPU:', torch.cuda.is_available())\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU: True\n",
      "train logs will save to: ./logs/vgg16/cifar10/regulated_training\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# args = parser.parse_args()\n",
    "\n",
    "# ======just for jupyter training purpose======\n",
    "# args = parser.parse_args('--dataset cifar10 --arch vgg --depth 16 --start-epoch 160 --epochs 3 --resume /home/lning/PyTorch/model/vgg16/checkpoint.pth.tar --no-cuda'.split())\n",
    "# args = parser.parse_args('--dataset cifar10 --arch vgg --depth 16 --start-epoch 0 --epochs 3 --no-cuda'.split())\n",
    "args = parser.parse_args('--dataset cifar10 --arch vgg --depth 16 --start-epoch 0 --epochs 1'.split())\n",
    "# ======just for jupyter training purpose======\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print('using GPU:', args.cuda)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# create logs folder \n",
    "args.save = os.path.join(args.save, args.arch+str(args.depth), args.dataset, 'regulated_training')\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "print('train logs will save to:', args.save)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "if args.dataset == 'cifar10':\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('./data.cifar10', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Pad(4),\n",
    "                           transforms.RandomCrop(32),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100('./data.cifar100', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Pad(4),\n",
    "                           transforms.RandomCrop(32),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.__dict__[args.arch](dataset=args.dataset, depth=args.depth)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {}) Prec1: {:f}\"\n",
    "              .format(args.resume, checkpoint['epoch'], best_prec1))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the quantized weight value to be -64 ~ 64. Downscale float weight accordingly\n",
    "def regulate_quantized_weight():\n",
    "    # print param and layer informations\n",
    "#     for module_full_name, module in model.named_modules():\n",
    "#         print (module_full_name)\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "#         if param_tensor == 'feature.0.float_weight':\n",
    "#             float_weight_p = model.state_dict()[param_tensor]\n",
    "#             print(float_weight_p)\n",
    "#         if param_tensor == 'feature.0.weight':\n",
    "#             weight_p = model.state_dict()[param_tensor]\n",
    "#             print(weight_p)\n",
    "#         if param_tensor == 'feature.1.weight':\n",
    "#             weight_n = model.state_dict()[param_tensor]\n",
    "#             print(weight_n)\n",
    "            \n",
    "    layer_id = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        name_part = param_tensor.split('.')\n",
    "        if 'float_weight' in name_part:\n",
    "            print(param_tensor)\n",
    "            quantized_weight_name = name_part[0]+'.'+name_part[1]+'.weight'\n",
    "            quantized_weight_scale = name_part[0]+'.'+name_part[1]+'.weight_scale'\n",
    "            quantized_weight_zero_point = name_part[0]+'.'+name_part[1]+'.weight_zero_point'\n",
    "            float_weight = model.state_dict()[param_tensor]\n",
    "            weight = model.state_dict()[quantized_weight_name]\n",
    "            weight_scale = model.state_dict()[quantized_weight_scale]\n",
    "            weight_zero_point = model.state_dict()[quantized_weight_zero_point]\n",
    "#             if param_tensor == 'feature.0.float_weight':\n",
    "#                 print('float_weight\\n', float_weight)\n",
    "#                 print('weight\\n',weight)\n",
    "#                 print('scale\\n',weight_scale)\n",
    "#                 print('zero_point\\n', weight_zero_point)\n",
    "        \n",
    "            # regulate the weight\n",
    "            weight_size = model.state_dict()[param_tensor].size()\n",
    "            if np.size(weight_size) > 2:\n",
    "                c = weight_size[0]\n",
    "                h = weight_size[1]\n",
    "                w = weight_size[2]\n",
    "                change_idx_list_l = np.nonzero(torch.round(weight_scale * weight - weight_zero_point) > 64)\n",
    "                change_idx_list_s = np.nonzero(torch.round(weight_scale * weight - weight_zero_point) < -64)\n",
    "                for idx in change_idx_list_l:\n",
    "                    if idx[0]*c+idx[1]*h+idx[2]*w % 8 != 7:\n",
    "                        # scale the float_weight accordingly\n",
    "                        float_weight[idx[0],idx[1],idx[2],idx[3]] = float_weight[idx[0],idx[1],idx[2],idx[3]] * \\\n",
    "                                torch.round(weight_scale * weight[idx[0],idx[1],idx[2],idx[3]] - weight_zero_point) / 64.\n",
    "                        # down write the value to 64\n",
    "                        weight[idx[0],idx[1],idx[2],idx[3]] = (64 + weight_zero_point)/weight_scale\n",
    "                for idx in change_idx_list_s:\n",
    "                    if idx[0]*c+idx[1]*h+idx[2]*w % 8 != 7:\n",
    "                        # scale the float_weight accordingly\n",
    "                        float_weight[idx[0],idx[1],idx[2],idx[3]] = float_weight[idx[0],idx[1],idx[2],idx[3]] * \\\n",
    "                                torch.round(weight_scale * weight[idx[0],idx[1],idx[2],idx[3]] - weight_zero_point) / (-64.)\n",
    "                        # down write the value to 64\n",
    "                        weight[idx[0],idx[1],idx[2],idx[3]] = (-64 + weight_zero_point)/weight_scale\n",
    "            if np.size(weight_size) <= 2:\n",
    "                c = weight_size[0]\n",
    "                change_idx_list_l = np.nonzero(torch.round(weight_scale * weight - weight_zero_point) > 64)\n",
    "                change_idx_list_s = np.nonzero(torch.round(weight_scale * weight - weight_zero_point) < -64)\n",
    "                for idx in change_idx_list_l:\n",
    "                    if idx[0]*c % 8 != 7:\n",
    "                        # scale the float_weight accordingly\n",
    "                        float_weight[idx[0],idx[1]] = float_weight[idx[0],idx[1]] * \\\n",
    "                                torch.round(weight_scale * weight[idx[0],idx[1]] - weight_zero_point) / 64.\n",
    "                        # down write the value to 64\n",
    "                        weight[idx[0],idx[1]] = (64 + weight_zero_point)/weight_scale\n",
    "                for idx in change_idx_list_s:\n",
    "                    if idx[0]*c % 8 != 7:\n",
    "                        # scale the float_weight accordingly\n",
    "                        float_weight[idx[0],idx[1]] = float_weight[idx[0],idx[1]] * \\\n",
    "                                torch.round(weight_scale * weight[idx[0],idx[1]] - weight_zero_point) / (-64.)\n",
    "                        # down write the value to 64\n",
    "                        weight[idx[0],idx[1]] = (-64 + weight_zero_point)/weight_scale\n",
    "#             for idx in change_idx_list:\n",
    "#                 if idx[0]*c+idx[1]*h+idx[2]*w % 8 != 7:\n",
    "#                     # scale the float_weight accordingly\n",
    "#                     float_weight[idx[0],idx[1],idx[2],idx[3]] = float_weight[idx[0],idx[1],idx[2],idx[3]] * \\\n",
    "#                             torch.round(weight_scale * weight[idx[0],idx[1],idx[2],idx[3]] - weight_zero_point) / 64.\n",
    "#                     # down write the value to 64\n",
    "#                     weight[idx[0],idx[1],idx[2],idx[3]] = (64 + weight_zero_point)/weight_scale\n",
    "# #                     print(idx, weight_scale, weight_zero_point)\n",
    "# #                     print(model.state_dict()[quantized_weight_name][0,0,0,1])\n",
    "# #                     print(torch.round(weight_scale * weight[0,0,0,1] - weight_zero_point))\n",
    "                    \n",
    "# #                     print((64 + weight_zero_point)/weight_scale, model.state_dict()[quantized_weight_name][0,0,0,1]) \n",
    "# #                     print(weight[0,0,0,1])\n",
    "# #             print(c,h,w)\n",
    "# #             print(torch.round(weight_scale * weight[0,0,0,1] - weight_zero_point))\n",
    "# #             print(np.nonzero(torch.round(weight_scale * weight - weight_zero_point) > 66))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regulate_train(epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    train_acc = 0.\n",
    "    print_flag = True\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.to(torch.device('cuda')), target.to(torch.device('cuda'))\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        compression_scheduler.on_minibatch_begin(epoch, batch_idx, optimizer)\n",
    "#         regulate_quantized_weight()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         loss = F.cross_entropy(output, target)\n",
    "        loss = regulate_loss(output, target)\n",
    "    \n",
    "        avg_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        compression_scheduler.on_minibatch_end(epoch, batch_idx, 800)\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regulate_loss(output, target):\n",
    "    print('weight_list\\n',weight_name_list) \n",
    "    layer_num = len(weight_name_list)\n",
    "    alpha = 0.00005\n",
    "    beta = 0.0001\n",
    "    norm_sum = F.cross_entropy(output, target)\n",
    "    for layer in np.arange(layer_num):\n",
    "        print(layer)\n",
    "        print(len(weight_list))\n",
    "        print(len(Z_list))\n",
    "        print(len(U_list))\n",
    "        norm_sum = norm_sum + alpha * torch.norm(weight_list[layer], 2)\n",
    "        item = weight_list[layer] - Z_list[layer] + U_list[layer]\n",
    "        norm_sum = norm_sum + beta * torch.norm(item, 2)\n",
    "    return norm_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    train_acc = 0.\n",
    "    print_flag = True\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.to(torch.device('cuda')), target.to(torch.device('cuda'))\n",
    "        data, target = Variable(data), Variable(target)\n",
    "#         quantizer = distiller.quantization.QuantAwareTrainRangeLinearQuantizer(model,optimizer)\n",
    "#         quantizer.quantize_params()\n",
    "        compression_scheduler.on_minibatch_begin(epoch, batch_idx, optimizer)\n",
    "        regulate_quantized_weight()\n",
    "        if print_flag:\n",
    "            regulate_quantized_weight()\n",
    "            print_flag = False\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        avg_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        quantizer.quantize_params()\n",
    "        compression_scheduler.on_minibatch_end(epoch, batch_idx, 800)\n",
    "\n",
    "#         if epoch == args.start_epoch and batch_idx == 1:\n",
    "#             # check the distribution of parameters \n",
    "#             thr = 16\n",
    "#             layer_id = 0 \n",
    "#             for param_name, param in model.named_parameters():\n",
    "#                 if len(param.size()) < 2:\n",
    "#                     continue\n",
    "#                 counter = collections.Counter(np.abs(param.data.cpu().numpy().ravel())//thr)\n",
    "#                 tmp = sorted(counter.items(), key=lambda x: x[0])\n",
    "#                 values, counts = zip(*tmp)\n",
    "#                 percentages = [count/sum(list(counts)) for count in counts]\n",
    "#                 bar = plt.bar(values, percentages)\n",
    "#                 for rect in bar:\n",
    "#                     height = rect.get_height()\n",
    "#                     plt.text(rect.get_x() + rect.get_width()/2.0, height, '%.4f%%' %(height*100), ha='center', va='bottom')\n",
    "#             #     print(['%.2f' %(p) for p in percentages])\n",
    "#                 #plt.hist(param.data.cpu().numpy().ravel(), bins=10, density=True)\n",
    "#                 plt.xticks(values, [str(int(v)*thr+thr) for v in values])\n",
    "#                 plt.title('layer_id:'+str(layer_id) + ', '+ str(tuple(param.size())))\n",
    "#             #     plt.grid()\n",
    "#                 plt.ylim(0, 1.1)\n",
    "#                 plt.show()\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "#     correct = 0\n",
    "    test_time = time.time() \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').data # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_time = time.time() - time.time() \n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    # print(correct, len(test_loader.dataset))\n",
    "    return correct*1.0 / len(test_loader.dataset)\n",
    "\n",
    "def save_checkpoint(state, is_best, filepath):\n",
    "    torch.save(state, os.path.join(filepath, 'checkpoint.pth.tar'))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(filepath, 'checkpoint.pth.tar'), os.path.join(filepath, 'model_best.pth.tar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(weight, weight_scale, weight_zero_point, target):\n",
    "    weight_size = weight.size()\n",
    "    upper_value = (64 + weight_zero_point)/weight_scale\n",
    "    lower_value = (-64 + weight_zero_point)/weight_scale\n",
    "#     print(np.size(weight_size))\n",
    "    if np.size(weight_size) > 2:\n",
    "        c = weight_size[0]\n",
    "        h = weight_size[1]\n",
    "        w = weight_size[2]\n",
    "        change_idx_list_l = np.nonzero(torch.round(weight_scale * weight - weight_zero_point) > 64)\n",
    "        change_idx_list_s = np.nonzero(torch.round(weight_scale * weight - weight_zero_point) < -64)\n",
    "        for idx in change_idx_list_l:\n",
    "            if idx[0]*c+idx[1]*h+idx[2]*w % 8 != 7:\n",
    "                # down write the value to 64\n",
    "                weight[idx[0],idx[1],idx[2],idx[3]] = upper_value\n",
    "        for idx in change_idx_list_s:\n",
    "            if idx[0]*c+idx[1]*h+idx[2]*w % 8 != 7:\n",
    "                # down write the value to 64\n",
    "                weight[idx[0],idx[1],idx[2],idx[3]] = lower_value\n",
    "    if np.size(weight_size) <= 2:\n",
    "        c = weight_size[0]\n",
    "        change_idx_list_l = np.nonzero(torch.round(weight_scale * weight - weight_zero_point) > 64)\n",
    "        print(change_idx_list_l)\n",
    "        change_idx_list_s = np.nonzero(torch.round(weight_scale * weight - weight_zero_point) < -64)\n",
    "        for idx in change_idx_list_l:\n",
    "            if idx[0]*c % 8 != 7:\n",
    "                # down write the value to 64\n",
    "                weight[idx[0],idx[1]] = upper_value\n",
    "        for idx in change_idx_list_s:\n",
    "            if idx[0]*c % 8 != 7:\n",
    "                # down write the value to 64\n",
    "                weight[idx[0],idx[1]] = lower_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regulate_param_init(weight_name_list, weight_list, Z_list, U_list, target):\n",
    "    # TODO: see if we want to use the quantized weight or float weight here for the regularization\n",
    "    layer_id = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        name_part = param_tensor.split('.')\n",
    "        if 'float_weight' in name_part:\n",
    "            quantized_weight_name = name_part[0]+'.'+name_part[1]+'.weight'\n",
    "            quantized_weight_scale = name_part[0]+'.'+name_part[1]+'.weight_scale'\n",
    "            quantized_weight_zero_point = name_part[0]+'.'+name_part[1]+'.weight_zero_point'\n",
    "            \n",
    "            weight_name_list.append(quantized_weight_name)\n",
    "            weight = model.state_dict()[quantized_weight_name]\n",
    "            weight_scale = model.state_dict()[quantized_weight_scale]\n",
    "            weight_zero_point = model.state_dict()[quantized_weight_zero_point]\n",
    "            \n",
    "            weight_list.append(weight)\n",
    "#             print(weight_list)\n",
    "            Z = weight.clone().detach()\n",
    "            \n",
    "#             print('here1')\n",
    "#             Z_list.append(projection(Z, weight_scale, weight_zero_point, target))\n",
    "            Z_list.append()\n",
    "#             print('here2')\n",
    "            U = np.zeros_like(Z)\n",
    "            U_list.append(U)\n",
    "            print(layer_id)\n",
    "            layer_id = layer_id + 1\n",
    "            \n",
    "    print('weight_list\\n',weight_name_list) \n",
    "    \n",
    "    print(len(weight_name_list))\n",
    "    print(len(weight_list))\n",
    "    print(len(Z_list))\n",
    "    print(len(U_list))\n",
    "    print('layer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regulate_param_update(weight_name_list, weight_list, Z_list, U_list, target):\n",
    "    weight_name_list = []\n",
    "    weight_list = []\n",
    "    Z_list_old = Z_list.copy()\n",
    "    Z_list = []\n",
    "    U_list_old = U_list.copy()\n",
    "    U_list = []\n",
    "    \n",
    "    idx = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        name_part = param_tensor.split('.')\n",
    "        if 'float_weight' in name_part:\n",
    "            quantized_weight_name = name_part[0]+'.'+name_part[1]+'.weight'\n",
    "            quantized_weight_scale = name_part[0]+'.'+name_part[1]+'.weight_scale'\n",
    "            quantized_weight_zero_point = name_part[0]+'.'+name_part[1]+'.weight_zero_point'\n",
    "            \n",
    "            weight_name_list.append(quantized_weight_name)\n",
    "            weight = model.state_dict()[quantized_weight_name]\n",
    "            weight_scale = model.state_dict()[quantized_weight_scale]\n",
    "            weight_zero_point = model.state_dict()[quantized_weight_zero_point]\n",
    "            \n",
    "            weight_list.append(weight)\n",
    "            Z = weight + torch.from_numpy(U_list_old[idx])\n",
    "#             Z = projection(Z, weight_scale, weight_zero_point, target)\n",
    "            Z_list.append(Z)\n",
    "            U = torch.from_numpy(U_list_old[idx]) + weight - Z\n",
    "            U_list.append(U)\n",
    "            idx += 1\n",
    "    print('weight_list\\n',weight_name_list) \n",
    "    print('idx: ', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "file_config() takes from 3 to 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a245101db1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m compression_scheduler = distiller.file_config(model,optimizer, \n\u001b[1;32m      3\u001b[0m                                     \u001b[0;34m'/home/lning/PyTorch/rethinking-network-pruning/cifar/l1-norm-pruning/quant_aware_training.yaml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                               compression_scheduler, (args.start_epoch-1) if args.resume else None)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbest_prec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file_config() takes from 3 to 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "compression_scheduler = distiller.CompressionScheduler(model) \n",
    "compression_scheduler = distiller.file_config(model,optimizer, \n",
    "                                    '/home/lning/PyTorch/rethinking-network-pruning/cifar/l1-norm-pruning/quant_aware_training.yaml', \n",
    "                                              compression_scheduler, (args.start_epoch-1) if args.resume else None)\n",
    "\n",
    "best_prec1 = 0.\n",
    "print(args.start_epoch)\n",
    "print(args.epochs)\n",
    "enter = 0\n",
    "weight_name_list = []\n",
    "weight_list = []\n",
    "Z_list = []\n",
    "U_list = []\n",
    "target = [-64, 64]\n",
    "for epoch in range(args.start_epoch, args.start_epoch+args.epochs):\n",
    "    compression_scheduler.on_epoch_begin(epoch)\n",
    "#     quantizer = distiller.quantization.QuantAwareTrainRangeLinearQuantizer(model,optimizer,bits_activations=8,\n",
    "#                                                                   bits_weights=8,\n",
    "#                                                                   bits_bias=8,\n",
    "#                                                                   overrides=None)\n",
    "#     quantizer.prepare_model()\n",
    "    # initialize the regulated parameters\n",
    "    print('initialize the regulated parametes')\n",
    "    regulate_param_init(weight_name_list, weight_list, Z_list, U_list, target)\n",
    "\n",
    "    if epoch in [args.epochs*0.5, args.epochs*0.75]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1\n",
    "    train_time = time.time()\n",
    "    print('begin the regulated training')\n",
    "    regulate_train(epoch)\n",
    "#     train(epoch)\n",
    "    \n",
    "    # compute regularization parameters\n",
    "    regulate_param_update(weight_name_list, weight_list, Z_list, U_list, target)\n",
    "        \n",
    "    train_time = time.time() - train_time \n",
    "    prec1 = test()\n",
    "    # print(prec1)\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'cfg': model.cfg\n",
    "    }, is_best, filepath=args.save)\n",
    "    compression_scheduler.on_epoch_end(epoch,optimizer)\n",
    "    print('Epoch: %d, train_time: %.2f (min), prec1: %f, best_prec1: %f\\n' %(epoch, train_time/60.0, prec1, best_prec1))\n",
    "\n",
    "# with open(os.path.join(args.save, 'train.txt'), 'w') as f:\n",
    "#     f.write('Epoch: %d, train_time: %.2f (min), prec1: %f, best_prec1: %f\\n' %(epoch, train_time/60.0, prec1, best_prec1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 63*64+2*3+2*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a%8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
